# Implementation Plan: Retrieval Pipeline Testing & Validation

**Feature**: Retrieval Pipeline Testing & Validation (Spec 2)
**Feature Branch**: `2-retrieval-testing`
**Created**: 2025-12-11
**Target Audience**: AI engineers, QA engineers, backend developers

---

## Executive Summary

This plan outlines the implementation strategy for a comprehensive retrieval testing suite that validates the vector database quality and retrieval accuracy from Spec 1 (Ingestion Pipeline). The system will provide interactive query execution, batch testing, performance measurement, and detailed reporting capabilities to ensure the RAG pipeline is production-ready before integration with the chatbot agent.

**Key Goals**:
- âœ… Execute semantic queries against Qdrant vector database with configurable parameters
- âœ… Validate semantic accuracy of retrieved results (â‰¥80% relevance for top-5)
- âœ… Measure and monitor query performance (p95 latency <500ms)
- âœ… Support batch testing with automated reporting
- âœ… Handle edge cases gracefully
- âœ… Provide comprehensive audit trails

---

## Scope & Dependencies

### In Scope
1. Interactive query execution against Qdrant
2. Result validation and semantic accuracy assessment
3. Performance and concurrency testing
4. Batch testing and automated reporting
5. Comprehensive logging and audit trails
6. Edge case handling
7. Configuration management (top-k, similarity thresholds, etc.)

### Out of Scope
- Query rewriting or reformulation
- Result re-ranking or alternative algorithms
- Caching or query prediction
- Integration with chatbot (testing vector DB in isolation)
- A/B testing embedding models
- Multi-language support

### External Dependencies
- **Qdrant Cloud**: Populated collection with 200+ vectors from Spec 1
- **Cohere API**: Query embedding generation (embed-english-v3.0, 1024-dimensional)
- **Python 3.8+**: Runtime environment
- **qdrant-client**: Vector database client library
- **cohere**: Python SDK for embeddings

### Internal Dependencies
- **Spec 1 (Ingestion)**: Produces the vectors to test
- **Spec 3 (Agent & API)**: Will consume validated retrieval quality
- **.env configuration**: API keys for Cohere and Qdrant

---

## Architecture & Design

### System Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Retrieval Testing System                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Query Execution Layer                              â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ â€¢ QueryExecutor: Execute single queries            â”‚ â”‚
â”‚ â”‚ â€¢ ConcurrentQueryRunner: Multi-threaded execution  â”‚ â”‚
â”‚ â”‚ â€¢ BatchQueryProcessor: Batch file processing       â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                          â†“                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Embedding & Retrieval Layer                        â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ â€¢ CohereEmbedder: Query embedding generation       â”‚ â”‚
â”‚ â”‚ â€¢ QdrantRetriever: Vector search with parameters   â”‚ â”‚
â”‚ â”‚ â€¢ ResultMapper: Parse Qdrant responses             â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                          â†“                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Validation & Analysis Layer                        â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ â€¢ SemanticValidator: Manual accuracy assessment    â”‚ â”‚
â”‚ â”‚ â€¢ ConsistencyChecker: Result determinism           â”‚ â”‚
â”‚ â”‚ â€¢ PerformanceAnalyzer: Latency metrics             â”‚ â”‚
â”‚ â”‚ â€¢ CoverageAnalyzer: Module distribution            â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                          â†“                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Reporting & Output Layer                           â”‚ â”‚
â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚ â”‚ â€¢ ReportGenerator: Batch test reports              â”‚ â”‚
â”‚ â”‚ â€¢ MetricsAggregator: Performance & accuracy stats  â”‚ â”‚
â”‚ â”‚ â€¢ AuditLogger: Query execution trail               â”‚ â”‚
â”‚ â”‚ â€¢ CLIInterface: Interactive query execution        â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“ Queries â†’ Cohere API â†’ Qdrant Cloud â†‘
```

### Data Flow

```
User Query (CLI/Batch)
        â†“
[Query Validation]
        â†“
[Cohere Embedding] â†’ Query Vector
        â†“
[Qdrant Search] â†’ Top-k Results with Scores
        â†“
[Result Mapping] â†’ SearchResult Objects
        â†“
[Consistency Check] â†’ Verify determinism
        â†“
[Semantic Analysis] â†’ Manual validation
        â†“
[Report Generation] â†’ Metrics & Summary
        â†“
Output (Console/File)
```

### Key Entities

```
Query
â”œâ”€ text: str (1-1000 chars)
â”œâ”€ top_k: int (default 5, range 1-100)
â”œâ”€ min_score: float (default 0.0, range 0.0-1.0)
â”œâ”€ module_filter: Optional[str] (filter by module)
â””â”€ timestamp: datetime

SearchResult
â”œâ”€ document_id: str
â”œâ”€ source_url: str
â”œâ”€ section_name: str
â”œâ”€ relevance_score: float
â”œâ”€ text_preview: str (max 1000 chars)
â”œâ”€ metadata: dict
â””â”€ rank: int

QueryExecution
â”œâ”€ query_id: str (unique identifier)
â”œâ”€ query: Query
â”œâ”€ results: List[SearchResult]
â”œâ”€ latency_ms: float
â”œâ”€ status: str (success/error)
â”œâ”€ error_message: Optional[str]
â””â”€ timestamp: datetime

TestBatch
â”œâ”€ batch_id: str
â”œâ”€ queries: List[Query]
â”œâ”€ execution_records: List[QueryExecution]
â”œâ”€ total_latency_ms: float
â””â”€ generated_at: datetime

ValidationReport
â”œâ”€ batch_id: str
â”œâ”€ total_queries: int
â”œâ”€ successful_queries: int
â”œâ”€ avg_latency_ms: float
â”œâ”€ p50_latency_ms: float
â”œâ”€ p95_latency_ms: float
â”œâ”€ p99_latency_ms: float
â”œâ”€ semantic_accuracy: float (0.0-1.0)
â”œâ”€ coverage_by_module: dict
â”œâ”€ edge_cases_handled: int
â””â”€ generated_at: datetime
```

---

## Technical Design

### Configuration Management (.env)

```env
# Cohere Configuration
COHERE_API_KEY=<your-cohere-api-key>
COHERE_MODEL=embed-english-v3.0

# Qdrant Configuration
QDRANT_URL=https://<your-qdrant-instance>.qdrant.io:6333
QDRANT_API_KEY=<your-qdrant-api-key>
QDRANT_COLLECTION_NAME=robotics_knowledge_base

# Query Execution Settings
DEFAULT_TOP_K=5
DEFAULT_MIN_SCORE=0.0
QUERY_TIMEOUT_SECONDS=5
CONCURRENT_QUERY_LIMIT=20

# Batch Testing
BATCH_FILE_FORMAT=csv  # csv or json
MAX_QUERIES_PER_BATCH=100
SEMANTIC_ACCURACY_THRESHOLD=0.80

# Logging
LOG_LEVEL=INFO
LOG_FILE=retrieval_tests.log
AUDIT_LOG_FILE=query_audit.log
```

### Query Execution Workflow

**Phase 1: Single Query Execution**
1. User submits query text via CLI or API
2. Query validation (length, special chars, encoding)
3. Generate embedding using Cohere API
4. Search Qdrant with top-k and min_score parameters
5. Map results to SearchResult objects
6. Log execution with latency
7. Display results with metadata
8. Store audit trail

**Phase 2: Concurrent Query Testing**
1. Load list of queries
2. Execute queries in thread pool (configurable workers)
3. Collect results as they complete
4. Measure individual latencies
5. Verify no dropped or duplicated results
6. Generate performance metrics
7. Report success/failure rates

**Phase 3: Batch Testing & Reporting**
1. Load test batch file (CSV or JSON)
2. Execute all queries sequentially or concurrently
3. Collect all execution records
4. Generate comprehensive report including:
   - Total queries executed
   - Success rate
   - Latency percentiles (p50, p95, p99)
   - Semantic accuracy (from manual spot-checks)
   - Coverage by module
   - Edge case handling summary
5. Output report to file (JSON/Markdown)
6. Log all queries to audit trail

### API Contracts

**Interactive Query Interface (CLI)**
```
Command: query <text> [--top-k K] [--min-score SCORE] [--module MODULE]

Input:
  text: str (query text)
  top_k: int (optional, default 5)
  min_score: float (optional, default 0.0)
  module: str (optional, filter results)

Output:
  Query executed successfully in XXms

  Results (top-k=5):
  1. [Relevance: 0.92] Module 1 - Robotics Basics
     Source: https://docs.example.com/robotics-basics
     Preview: "Robotics is the field of..."

  2. [Relevance: 0.87] Module 2 - Robot Learning
     Source: https://docs.example.com/robot-learning
     Preview: "Learning algorithms enable robots..."

  ... (more results)
```

**Batch Test Interface**
```
Command: batch-test <file> [--format csv|json] [--output report.json]

Input File (CSV):
  query,expected_category,top_k,min_score
  "What is robotics?",intro,5,0.0
  "How do robots learn?",learning,5,0.5
  "Robot perception methods",perception,10,0.3
  ...

Output Report (JSON):
  {
    "batch_id": "batch_20251211_001",
    "timestamp": "2025-12-11T15:30:00Z",
    "total_queries": 20,
    "successful_queries": 20,
    "failed_queries": 0,
    "latency_metrics": {
      "avg_ms": 245,
      "p50_ms": 230,
      "p95_ms": 380,
      "p99_ms": 450
    },
    "semantic_accuracy": 0.85,
    "coverage": {
      "Module 1": 5,
      "Module 2": 8,
      "Module 3": 4,
      "Module 4": 3
    },
    "edge_cases": 7,
    "generated_at": "2025-12-11T15:35:00Z"
  }
```

---

## Implementation Phases

### Phase 1: Core Query Execution (Weeks 1-2)
**Deliverables**:
- [ ] QueryExecutor class with Cohere embedding
- [ ] QdrantRetriever with top-k and similarity filtering
- [ ] SearchResult data model
- [ ] CLI interface for single query execution
- [ ] Latency measurement and logging
- [ ] Unit tests for query execution
- [ ] Configuration via .env file

**Success Metrics**:
- Single queries execute successfully
- Latency <500ms p95
- All SearchResult fields populated correctly
- CLI interface functional

### Phase 2: Validation & Analysis (Weeks 2-3)
**Deliverables**:
- [ ] SemanticValidator for accuracy assessment
- [ ] ConsistencyChecker for determinism validation
- [ ] PerformanceAnalyzer for latency metrics
- [ ] CoverageAnalyzer for module distribution
- [ ] Query audit logging
- [ ] Edge case handling (empty DB, long queries, special chars)
- [ ] Unit and integration tests

**Success Metrics**:
- Validation tools functional
- All edge cases handled gracefully
- 100% query audit trail
- Consistency verified (same query = same results)

### Phase 3: Batch Testing & Reporting (Weeks 3-4)
**Deliverables**:
- [ ] BatchQueryProcessor for CSV/JSON file loading
- [ ] ConcurrentQueryRunner for multi-threaded execution
- [ ] ReportGenerator with metrics aggregation
- [ ] Batch test CLI command
- [ ] Report output formats (JSON, Markdown)
- [ ] Integration tests with sample batch files
- [ ] Documentation and examples

**Success Metrics**:
- Batch testing processes 50+ queries successfully
- Report generation includes all required metrics
- Concurrent query execution (20 simultaneous) succeeds
- Reports generated in <30 seconds for 50-query batch

### Phase 4: Testing & Documentation (Weeks 4-5)
**Deliverables**:
- [ ] Comprehensive test suite (unit + integration)
- [ ] Test batch files with expected categories
- [ ] User documentation (README with examples)
- [ ] Developer documentation (architecture, extending)
- [ ] Performance profiling and optimization
- [ ] Error handling and recovery tests
- [ ] Ready for production deployment

**Success Metrics**:
- All success criteria (SC-001 to SC-010) validated
- Test coverage >85%
- Documentation complete
- All requirements met

---

## Success Criteria Implementation

| Criterion | Implementation Strategy |
|-----------|------------------------|
| **SC-001**: p95 latency <500ms | Monitor Cohere API + Qdrant search time; optimize batch size |
| **SC-002**: 100 consecutive queries, 100% success | ConcurrentQueryRunner with error handling and retry logic |
| **SC-003**: 20 concurrent queries, zero drops | Thread-safe result collection, connection pooling |
| **SC-004**: â‰¥80% semantic relevance | Manual spot-check utilities; semantic validator tool |
| **SC-005**: Result consistency | ConsistencyChecker compares multiple executions |
| **SC-006**: Coverage across modules | CoverageAnalyzer aggregates module distribution |
| **SC-007**: 50+ predefined queries | Sample batch file with test suite |
| **SC-008**: Report metrics (p50/p95/p99, coverage, accuracy) | ReportGenerator aggregates all metrics |
| **SC-009**: All 7 edge cases handled | Explicit handlers for each edge case |
| **SC-010**: 100% audit trail | AuditLogger records all queries and results |

---

## Risk Analysis & Mitigation

### Top 3 Risks

**Risk 1: Qdrant Connection Instability**
- **Impact**: Tests fail randomly; results unreliable
- **Mitigation**: Implement connection retry logic (exponential backoff), health checks, fallback error messages
- **Monitoring**: Track connection failures in logs; alert on threshold

**Risk 2: Cohere API Rate Limiting**
- **Impact**: Batch tests fail when hitting rate limits
- **Mitigation**: Implement request throttling, batch embedding API calls, respect rate limit headers
- **Monitoring**: Log rate limit events; adjust concurrency based on API response

**Risk 3: Semantic Accuracy Validation is Manual**
- **Impact**: Scalability issue; spot-checks don't catch systematic problems
- **Mitigation**: Provide tools for efficient manual validation; document process; consider heuristic scoring
- **Monitoring**: Track validation feedback; identify patterns in low-accuracy results

---

## Operational Readiness

### Deployment Checklist
- [ ] .env file configured with valid API keys
- [ ] Qdrant collection populated from Spec 1 ingestion
- [ ] Python dependencies installed (qdrant-client, cohere, etc.)
- [ ] Sample test batch file created
- [ ] Logs directory created and writable
- [ ] Database backup before first production run

### Monitoring & Alerts
```
Metrics to track:
- Query execution latency (p50, p95, p99)
- API error rates (Cohere, Qdrant)
- Connection failures and retries
- Batch processing times
- Semantic accuracy trend

Alert thresholds:
- p95 latency >500ms (warning), >1000ms (critical)
- Error rate >5% (warning), >10% (critical)
- Connection failures >3 in 1 hour (warning)
```

### Runbooks

**Issue**: Qdrant connection timeout
1. Check Qdrant Cloud status dashboard
2. Verify API key in .env file
3. Check network connectivity to Qdrant
4. Restart application
5. If persistent, escalate to Qdrant support

**Issue**: Cohere API rate limit hit
1. Check Cohere API usage dashboard
2. Reduce concurrent query limit in .env
3. Increase delay between API calls
4. Consider upgrading Cohere plan

**Issue**: Low semantic accuracy scores
1. Review sample queries and results
2. Check if results are actually relevant (validation may be too strict)
3. Profile Cohere embeddings quality
4. Consider different top-k values or similarity thresholds
5. Document findings for Spec 3 (Agent) integration

---

## Evaluation & Validation

### Definition of Done
- âœ… All FR-001 to FR-015 functional requirements implemented
- âœ… All SC-001 to SC-010 success criteria validated
- âœ… All 7 edge cases handled without crashes
- âœ… 100% audit trail logged
- âœ… Batch testing processes â‰¥50 queries successfully
- âœ… Reports generated with all required metrics
- âœ… Test coverage >85%
- âœ… Documentation complete
- âœ… Performance benchmarked and optimized
- âœ… Ready for Spec 3 (Agent & API) integration

### Testing Strategy

**Unit Tests**
- Query validation and embedding generation
- Result mapping and data structures
- Metric calculations and aggregations
- Error handling for edge cases

**Integration Tests**
- End-to-end query execution (Cohere â†’ Qdrant â†’ Results)
- Concurrent query execution with result verification
- Batch processing with multi-threaded execution
- Report generation from batch results

**Performance Tests**
- Single query latency <500ms p95
- Concurrent query handling (20 simultaneous)
- Batch processing speed (50 queries)
- Memory usage under load

**Validation Tests**
- Semantic accuracy spot-checks (manual)
- Result consistency (multiple executions)
- Coverage across all modules
- All edge cases triggered and handled

---

## Timeline & Milestones

| Week | Milestone | Deliverables |
|------|-----------|--------------|
| 1-2  | Phase 1: Core Query Execution | Query executor, CLI interface, logging |
| 2-3  | Phase 2: Validation & Analysis | Validators, consistency checking, edge cases |
| 3-4  | Phase 3: Batch Testing | Batch processor, report generator, CLI |
| 4-5  | Phase 4: Testing & Docs | Full test suite, documentation, optimization |

**Ready for**: Spec 3 (Agent & API) implementation and integration

---

## Next Steps

1. âœ… Specification complete and validated (20/20 checklist passing)
2. ðŸ”œ Implement Phase 1: Core query execution (start with infrastructure setup)
3. ðŸ”œ Complete integration testing with sample queries
4. ðŸ”œ Generate sample reports and validation documentation
5. ðŸ”œ Integrate with Spec 3 (Agent & API) when ready

---

## Key Decisions & Trade-offs

### Decision 1: Manual Semantic Validation
- **Choice**: Implement tools for manual spot-checks rather than automated scoring
- **Rationale**: Semantic relevance is domain-specific; manual review more reliable than heuristics
- **Trade-off**: Slower validation but higher confidence

### Decision 2: Thread-Based Concurrency
- **Choice**: Use Python threading for concurrent queries
- **Rationale**: Queries are I/O-bound (waiting for API responses); threading sufficient
- **Trade-off**: Simpler than async; Python GIL impact minimal for I/O

### Decision 3: Session-Level Batch Testing
- **Choice**: Process each batch independently without persistent state
- **Rationale**: Simplifies implementation; each batch is self-contained
- **Trade-off**: Can't easily track historical trends; mitigate by saving reports

### Decision 4: Configuration via .env
- **Choice**: Use environment variables for all configuration
- **Rationale**: Secure (no hardcoded secrets); flexible for different environments
- **Trade-off**: Requires .env file setup; mitigate with .env.example template

---

## Dependencies & Integration Points

```
Spec 2: Retrieval Testing
        â†“ (depends on)
Spec 1: Ingestion Pipeline
        â”œâ”€ Produces: 200-500 vectors in Qdrant
        â””â”€ Provides: Module structure and metadata

Spec 2 outputs to:
        â”œâ”€ Spec 3: Agent & API (validates retrieval quality)
        â””â”€ Validation Reports (confidence for production deployment)
```

---

**Status**: Ready for Phase 1 implementation
**Confidence Level**: High
